{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Pre-Trained Routing Disagreement Models\n",
        "\n",
        "**This is the recommended starting point for most users.**\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load the pre-trained models\n",
        "2. Generate the 6 required predictor features for your O-D pairs\n",
        "3. Predict where routing platforms will disagree\n",
        "4. Interpret and use the results\n",
        "\n",
        "## What the Models Predict\n",
        "\n",
        "- **Distance Model**: Predicts if platforms will disagree by >5% on walking distance\n",
        "- **Time Model**: Predicts if platforms will disagree by >20% on walking time\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Origin/destination coordinates (latitude, longitude)\n",
        "- Google Earth Engine account (free) for elevation data\n",
        "- US Census API key (free) for population data"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0: Setup"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install osmnx geopandas shapely pyproj geopy earthengine-api census pygris joblib pandas numpy tqdm -q\n",
        "\n",
        "# Clone the repository to get the pre-trained models\n",
        "!git clone https://github.com/Malmusidi/routing-prediction.git repo 2>/dev/null || echo \"Repository exists or using local files\"\n",
        "\n",
        "print(\"✓ Setup complete\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration { display-mode: \"form\" }\n",
        "\n",
        "# If you cloned the repo, models are in repo/models/\n",
        "# Otherwise, upload the model files and update these paths\n",
        "DISTANCE_MODEL_PATH = 'repo/models/distance_model.joblib'  #@param {type:\"string\"}\n",
        "TIME_MODEL_PATH = 'repo/models/time_model.joblib'  #@param {type:\"string\"}\n",
        "\n",
        "# API Keys (get these for free - links in README)\n",
        "CENSUS_API_KEY = 'YOUR_CENSUS_API_KEY'  #@param {type:\"string\"}\n",
        "GEE_PROJECT_ID = 'YOUR_GEE_PROJECT_ID'  #@param {type:\"string\"}\n",
        "\n",
        "# Census geography for your study area\n",
        "STATE_FIPS = '13'  #@param {type:\"string\"}\n",
        "COUNTY_FIPS = '059'  #@param {type:\"string\"}\n",
        "\n",
        "print(\"Configuration set!\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import osmnx as ox\n",
        "from shapely.geometry import Point\n",
        "from geopy.distance import geodesic\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure OSMnx\n",
        "ox.settings.log_console = False\n",
        "ox.settings.use_cache = True\n",
        "\n",
        "print(\"✓ Libraries imported\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load Pre-Trained Models"
      ],
      "metadata": {
        "id": "load_models"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained models\n",
        "try:\n",
        "    distance_model = joblib.load(DISTANCE_MODEL_PATH)\n",
        "    time_model = joblib.load(TIME_MODEL_PATH)\n",
        "    print(\"✓ Models loaded successfully!\")\n",
        "    print(f\"  Distance model: {distance_model.n_estimators} trees\")\n",
        "    print(f\"  Time model: {time_model.n_estimators} trees\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠️ Model files not found!\")\n",
        "    print(\"\\nOption 1: Upload the model files manually\")\n",
        "    print(\"Option 2: Update the model paths in the configuration above\")\n",
        "    print(\"Option 3: Clone the repository with the correct username\")"
      ],
      "metadata": {
        "id": "load_models_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Prepare Your O-D Pairs\n",
        "\n",
        "You can either:\n",
        "- **Upload a CSV** with your O-D coordinates\n",
        "- **Create sample data** to test the workflow"
      ],
      "metadata": {
        "id": "prepare_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose your data source { display-mode: \"form\" }\n",
        "\n",
        "data_source = \"Create sample data\"  #@param [\"Upload CSV\", \"Create sample data\"]\n",
        "\n",
        "if data_source == \"Upload CSV\":\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your CSV file...\")\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    df = pd.read_csv(filename)\n",
        "    print(f\"\\n✓ Loaded {len(df)} O-D pairs\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    \n",
        "else:\n",
        "    # Create sample O-D pairs in Athens-Clarke County, Georgia\n",
        "    # These are example coordinates for demonstration\n",
        "    sample_data = {\n",
        "        'origin_lat': [33.9519, 33.9612, 33.9478, 33.9550, 33.9680],\n",
        "        'origin_lon': [-83.3576, -83.3780, -83.3890, -83.3650, -83.3920],\n",
        "        'dest_lat': [33.9545, 33.9580, 33.9510, 33.9590, 33.9620],\n",
        "        'dest_lon': [-83.3620, -83.3750, -83.3850, -83.3700, -83.3880]\n",
        "    }\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    print(\"✓ Created sample data with 5 O-D pairs in Athens-Clarke County, GA\")\n",
        "\n",
        "# Set column names\n",
        "COL_ORIGIN_LAT = 'origin_lat'\n",
        "COL_ORIGIN_LON = 'origin_lon'\n",
        "COL_DEST_LAT = 'dest_lat'\n",
        "COL_DEST_LON = 'dest_lon'\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Generate Predictor Features\n",
        "\n",
        "The models require these 6 features:\n",
        "1. `Straight_Line_Distance_m`\n",
        "2. `Origin_Road_Length_Density_m_km2`\n",
        "3. `Dest_Intersection_Density_n_km2`\n",
        "4. `Slope_Pct`\n",
        "5. `Elevation_Difference_m`\n",
        "6. `Population`"
      ],
      "metadata": {
        "id": "generate_features"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "BUFFER_M = 400\n",
        "CRS_METRIC = 3857\n",
        "\n",
        "# Helper functions\n",
        "def area_km2(geom):\n",
        "    return geom.area / 1_000_000\n",
        "\n",
        "def buffer_from_latlon(lat, lon):\n",
        "    pt = Point(lon, lat)\n",
        "    return gpd.GeoSeries([pt], crs=4326).to_crs(CRS_METRIC).buffer(BUFFER_M)[0]\n",
        "\n",
        "def calculate_haversine(row):\n",
        "    origin = (row[COL_ORIGIN_LAT], row[COL_ORIGIN_LON])\n",
        "    dest = (row[COL_DEST_LAT], row[COL_DEST_LON])\n",
        "    return geodesic(origin, dest).meters\n",
        "\n",
        "def net_metrics(buf_metric):\n",
        "    \"\"\"Calculate network metrics within buffer\"\"\"\n",
        "    intersect_density = road_len_density = 0.0\n",
        "    area = area_km2(buf_metric)\n",
        "    buf_wgs = gpd.GeoSeries([buf_metric], crs=CRS_METRIC).to_crs(4326).iloc[0]\n",
        "    \n",
        "    try:\n",
        "        G = ox.graph_from_polygon(buf_wgs, network_type=\"drive_service\", \n",
        "                                   simplify=True, retain_all=False)\n",
        "        if len(G.nodes) and len(G.edges):\n",
        "            nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
        "            edges_m = edges.to_crs(CRS_METRIC)\n",
        "            road_len_density = edges_m.geometry.length.sum() / area\n",
        "            \n",
        "            nodes_m = nodes.to_crs(CRS_METRIC)\n",
        "            nodes_cl = gpd.clip(nodes_m, buf_metric)\n",
        "            if \"street_count\" in nodes_cl.columns:\n",
        "                inter_cnt = (nodes_cl[\"street_count\"] >= 3).sum()\n",
        "            else:\n",
        "                deg = Counter([u for u, v, k in G.edges(keys=True)] +\n",
        "                            [v for u, v, k in G.edges(keys=True)])\n",
        "                inter_cnt = sum(1 for n in nodes_cl.index if deg.get(n, 0) >= 3)\n",
        "            intersect_density = inter_cnt / area\n",
        "    except:\n",
        "        pass\n",
        "    return intersect_density, road_len_density\n",
        "\n",
        "print(\"✓ Helper functions defined\")"
      ],
      "metadata": {
        "id": "helper_funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 1: Straight-line distance\n",
        "print(\"Calculating straight-line distances...\")\n",
        "df['Straight_Line_Distance_m'] = df.apply(calculate_haversine, axis=1)\n",
        "print(f\"✓ Distances: {df['Straight_Line_Distance_m'].min():.0f} - {df['Straight_Line_Distance_m'].max():.0f} m\")"
      ],
      "metadata": {
        "id": "feature1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features 2-3: Network metrics\n",
        "print(\"Calculating network metrics (this may take a moment)...\")\n",
        "\n",
        "df['Origin_Road_Length_Density_m_km2'] = 0.0\n",
        "df['Dest_Intersection_Density_n_km2'] = 0.0\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
        "    # Origin road density\n",
        "    buf_origin = buffer_from_latlon(row[COL_ORIGIN_LAT], row[COL_ORIGIN_LON])\n",
        "    _, rd_o = net_metrics(buf_origin)\n",
        "    df.at[i, 'Origin_Road_Length_Density_m_km2'] = rd_o\n",
        "    \n",
        "    # Destination intersection density\n",
        "    buf_dest = buffer_from_latlon(row[COL_DEST_LAT], row[COL_DEST_LON])\n",
        "    id_d, _ = net_metrics(buf_dest)\n",
        "    df.at[i, 'Dest_Intersection_Density_n_km2'] = id_d\n",
        "\n",
        "print(\"✓ Network metrics calculated\")"
      ],
      "metadata": {
        "id": "feature23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features 4-5: Topographic data from Google Earth Engine\n",
        "print(\"Fetching elevation data from Google Earth Engine...\")\n",
        "\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=GEE_PROJECT_ID)\n",
        "\n",
        "DEM = ee.Image('USGS/SRTMGL1_003')\n",
        "\n",
        "def get_elevation(lat, lon):\n",
        "    \"\"\"Get elevation at a point\"\"\"\n",
        "    point = ee.Geometry.Point(lon, lat)\n",
        "    elev = DEM.sample(point, 30).first().get('elevation').getInfo()\n",
        "    return elev if elev else 0\n",
        "\n",
        "# Get elevations\n",
        "df['elev_origin'] = df.apply(lambda r: get_elevation(r[COL_ORIGIN_LAT], r[COL_ORIGIN_LON]), axis=1)\n",
        "df['elev_dest'] = df.apply(lambda r: get_elevation(r[COL_DEST_LAT], r[COL_DEST_LON]), axis=1)\n",
        "\n",
        "# Calculate derived features\n",
        "df['Elevation_Difference_m'] = df['elev_dest'] - df['elev_origin']\n",
        "df['Slope_Pct'] = (df['Elevation_Difference_m'] / df['Straight_Line_Distance_m']) * 100\n",
        "\n",
        "# Clean up temp columns\n",
        "df = df.drop(columns=['elev_origin', 'elev_dest'])\n",
        "\n",
        "print(\"✓ Topographic features calculated\")"
      ],
      "metadata": {
        "id": "feature45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 6: Population from Census\n",
        "print(\"Fetching population data from Census...\")\n",
        "\n",
        "from census import Census\n",
        "import pygris\n",
        "\n",
        "# Download block groups\n",
        "block_groups = pygris.block_groups(state=STATE_FIPS, county=COUNTY_FIPS, year=2020, cache=True)\n",
        "block_groups = block_groups.to_crs(CRS_METRIC)\n",
        "\n",
        "# Get population data\n",
        "c = Census(CENSUS_API_KEY)\n",
        "pop_data = c.pl.get(('P1_001N',), geo={'for': 'block group:*', \n",
        "                    'in': f'state:{STATE_FIPS} county:{COUNTY_FIPS}'}, year=2020)\n",
        "pop_df = pd.DataFrame(pop_data).rename(columns={'P1_001N': 'Population'})\n",
        "pop_df['GEOID'] = pop_df['state'] + pop_df['county'] + pop_df['tract'] + pop_df['block group']\n",
        "pop_df = pop_df[['GEOID', 'Population']].astype({'Population': float})\n",
        "\n",
        "# Merge and spatial join\n",
        "block_groups = block_groups.merge(pop_df, on='GEOID', how='left')\n",
        "block_groups['Population'] = block_groups['Population'].fillna(0)\n",
        "\n",
        "geometry = [Point(lon, lat) for lon, lat in zip(df[COL_ORIGIN_LON], df[COL_ORIGIN_LAT])]\n",
        "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=4326).to_crs(CRS_METRIC)\n",
        "gdf_joined = gdf.sjoin(block_groups[['GEOID', 'Population', 'geometry']], how='left', predicate='within')\n",
        "\n",
        "df['Population'] = gdf_joined['Population'].fillna(0).values\n",
        "\n",
        "print(\"✓ Population data added\")"
      ],
      "metadata": {
        "id": "feature6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify all features are present\n",
        "FEATURE_COLS = [\n",
        "    'Straight_Line_Distance_m',\n",
        "    'Origin_Road_Length_Density_m_km2',\n",
        "    'Dest_Intersection_Density_n_km2',\n",
        "    'Slope_Pct',\n",
        "    'Elevation_Difference_m',\n",
        "    'Population'\n",
        "]\n",
        "\n",
        "print(\"\\nFeature Summary:\")\n",
        "print(\"=\"*50)\n",
        "for col in FEATURE_COLS:\n",
        "    if col in df.columns:\n",
        "        print(f\"✓ {col:40} Mean: {df[col].mean():10.2f}\")\n",
        "    else:\n",
        "        print(f\"✗ {col:40} MISSING\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"\\n✓ All features ready for prediction!\")"
      ],
      "metadata": {
        "id": "verify_features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Make Predictions"
      ],
      "metadata": {
        "id": "predict"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare feature matrix\n",
        "X = df[FEATURE_COLS]\n",
        "\n",
        "# Make predictions\n",
        "df['distance_disagreement'] = distance_model.predict(X)\n",
        "df['time_disagreement'] = time_model.predict(X)\n",
        "\n",
        "# Get prediction probabilities\n",
        "df['distance_disagreement_prob'] = distance_model.predict_proba(X)[:, 1]\n",
        "df['time_disagreement_prob'] = time_model.predict_proba(X)[:, 1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal O-D pairs analyzed: {len(df)}\")\n",
        "print(f\"\\nDistance Disagreement (>5%):\")\n",
        "print(f\"  Predicted to agree: {(df['distance_disagreement']==0).sum()}\")\n",
        "print(f\"  Predicted to disagree: {(df['distance_disagreement']==1).sum()}\")\n",
        "print(f\"\\nTime Disagreement (>20%):\")\n",
        "print(f\"  Predicted to agree: {(df['time_disagreement']==0).sum()}\")\n",
        "print(f\"  Predicted to disagree: {(df['time_disagreement']==1).sum()}\")"
      ],
      "metadata": {
        "id": "make_predictions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View results\n",
        "result_cols = [COL_ORIGIN_LAT, COL_ORIGIN_LON, COL_DEST_LAT, COL_DEST_LON,\n",
        "               'Straight_Line_Distance_m', 'distance_disagreement', \n",
        "               'distance_disagreement_prob', 'time_disagreement', \n",
        "               'time_disagreement_prob']\n",
        "\n",
        "print(\"\\nDetailed Results:\")\n",
        "df[result_cols]"
      ],
      "metadata": {
        "id": "view_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Interpret and Use Results\n",
        "\n",
        "### How to Use These Predictions\n",
        "\n",
        "**Option 1: Selective Multi-Platform Validation**\n",
        "- Query all 3 platforms only for routes where `disagreement = 1`\n",
        "- Use single-platform results for routes where `disagreement = 0`\n",
        "\n",
        "**Option 2: Uncertainty Reporting**\n",
        "- Report the percentage of high-disagreement routes as a measure of analytical uncertainty\n",
        "- Higher percentages indicate less reliable accessibility estimates"
      ],
      "metadata": {
        "id": "interpret"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate uncertainty metrics for your analysis\n",
        "pct_dist_disagree = (df['distance_disagreement'] == 1).mean() * 100\n",
        "pct_time_disagree = (df['time_disagreement'] == 1).mean() * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"UNCERTAINTY ASSESSMENT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nRoutes predicted to have platform disagreement:\")\n",
        "print(f\"  Distance (>5% discrepancy): {pct_dist_disagree:.1f}%\")\n",
        "print(f\"  Time (>20% discrepancy): {pct_time_disagree:.1f}%\")\n",
        "\n",
        "if pct_dist_disagree > 50 or pct_time_disagree > 50:\n",
        "    print(\"\\n⚠️ High uncertainty: Consider validating with multiple platforms\")\n",
        "elif pct_dist_disagree > 25 or pct_time_disagree > 25:\n",
        "    print(\"\\n⚡ Moderate uncertainty: Selective validation recommended\")\n",
        "else:\n",
        "    print(\"\\n✓ Low uncertainty: Single-platform results likely reliable\")"
      ],
      "metadata": {
        "id": "uncertainty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify routes that need validation\n",
        "routes_to_validate = df[\n",
        "    (df['distance_disagreement'] == 1) | (df['time_disagreement'] == 1)\n",
        "].copy()\n",
        "\n",
        "print(f\"\\nRoutes requiring multi-platform validation: {len(routes_to_validate)} / {len(df)}\")\n",
        "\n",
        "if len(routes_to_validate) > 0:\n",
        "    print(\"\\nThese routes are predicted to have significant platform disagreement:\")\n",
        "    display(routes_to_validate[[COL_ORIGIN_LAT, COL_ORIGIN_LON, \n",
        "                                 COL_DEST_LAT, COL_DEST_LON,\n",
        "                                 'distance_disagreement_prob', \n",
        "                                 'time_disagreement_prob']])"
      ],
      "metadata": {
        "id": "identify_validation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Export Results"
      ],
      "metadata": {
        "id": "export"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "output_file = '/content/routing_predictions.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\n✓ Results saved to {output_file}\")\n",
        "print(f\"\\nColumns in output file:\")\n",
        "for col in df.columns:\n",
        "    print(f\"  - {col}\")"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download results (Colab)\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"✓ File downloaded\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You've successfully:\n",
        "1. ✓ Loaded pre-trained routing disagreement models\n",
        "2. ✓ Generated predictor features for your O-D pairs\n",
        "3. ✓ Predicted where routing platforms will disagree\n",
        "4. ✓ Identified routes requiring multi-platform validation\n",
        "\n",
        "### Next Steps\n",
        "- For routes with predicted disagreement, query Google Maps, ArcGIS, and ORS\n",
        "- Reconcile estimates through averaging or manual validation\n",
        "- Report uncertainty metrics in your accessibility analysis\n",
        "\n",
        "### Questions?\n",
        "See the README or open an issue on GitHub."
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
