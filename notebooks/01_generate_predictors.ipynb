{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Predictor Features for Routing Disagreement Prediction\n",
        "\n",
        "This notebook calculates the 6 predictor features required for the routing disagreement models:\n",
        "\n",
        "1. **Straight_Line_Distance_m** - Haversine distance between origin and destination\n",
        "2. **Origin_Road_Length_Density_m_km2** - Road network density around origin\n",
        "3. **Dest_Intersection_Density_n_km2** - Intersection density around destination\n",
        "4. **Slope_Pct** - Slope percentage between origin and destination\n",
        "5. **Elevation_Difference_m** - Elevation change between origin and destination\n",
        "6. **Population** - Census block group population at origin\n",
        "\n",
        "## Requirements\n",
        "- CSV file with origin/destination coordinates (latitude, longitude)\n",
        "- Google Earth Engine account (free): https://earthengine.google.com/\n",
        "- US Census API key (free): https://api.census.gov/data/key_signup.html\n",
        "\n",
        "## Input Format\n",
        "Your CSV should have columns for origin and destination coordinates. Update the column names in the Configuration section below."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0: Install Dependencies"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install osmnx geopandas shapely pyproj tqdm pandas numpy geopy earthengine-api census us pygris -q\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Configuration\n",
        "\n",
        "**⚠️ UPDATE THESE SETTINGS FOR YOUR DATA**"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration Settings { display-mode: \"form\" }\n",
        "\n",
        "# ============================================================================\n",
        "# FILE PATHS - Update these for your environment\n",
        "# ============================================================================\n",
        "INPUT_CSV = '/content/your_od_pairs.csv'  #@param {type:\"string\"}\n",
        "OUTPUT_CSV = '/content/data_with_predictors.csv'  #@param {type:\"string\"}\n",
        "\n",
        "# ============================================================================\n",
        "# COLUMN NAMES - Update to match your CSV column names\n",
        "# ============================================================================\n",
        "COL_ORIGIN_LAT = 'origin_lat'  #@param {type:\"string\"}\n",
        "COL_ORIGIN_LON = 'origin_lon'  #@param {type:\"string\"}\n",
        "COL_DEST_LAT = 'dest_lat'  #@param {type:\"string\"}\n",
        "COL_DEST_LON = 'dest_lon'  #@param {type:\"string\"}\n",
        "\n",
        "# ============================================================================\n",
        "# API KEYS - Get these for free from the links above\n",
        "# ============================================================================\n",
        "CENSUS_API_KEY = 'YOUR_CENSUS_API_KEY_HERE'  #@param {type:\"string\"}\n",
        "GEE_PROJECT_ID = 'YOUR_GEE_PROJECT_ID'  #@param {type:\"string\"}\n",
        "\n",
        "# ============================================================================\n",
        "# CENSUS GEOGRAPHY - Update for your study area\n",
        "# Find FIPS codes at: https://www.census.gov/library/reference/code-lists/ansi.html\n",
        "# ============================================================================\n",
        "STATE_FIPS = '13'  #@param {type:\"string\"}\n",
        "COUNTY_FIPS = '059'  #@param {type:\"string\"}\n",
        "CENSUS_YEAR = 2020  #@param {type:\"integer\"}\n",
        "\n",
        "# ============================================================================\n",
        "# PROCESSING PARAMETERS (defaults should work for most cases)\n",
        "# ============================================================================\n",
        "BUFFER_M = 400  # Walking catchment buffer in meters\n",
        "CRS_METRIC = 3857  # EPSG:3857 Web Mercator for metric calculations\n",
        "BATCH_SIZE = 50  # Rows per batch for network metrics\n",
        "MAX_WORKERS = 4  # Parallel threads\n",
        "\n",
        "print(\"Configuration loaded!\")\n",
        "print(f\"  Input: {INPUT_CSV}\")\n",
        "print(f\"  Output: {OUTPUT_CSV}\")\n",
        "print(f\"  Study area: State {STATE_FIPS}, County {COUNTY_FIPS}\")"
      ],
      "metadata": {
        "id": "config_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Initialize Libraries and Authenticate"
      ],
      "metadata": {
        "id": "init"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import osmnx as ox\n",
        "from shapely.geometry import Point\n",
        "from geopy.distance import geodesic\n",
        "from tqdm.auto import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import Counter\n",
        "import ee\n",
        "from census import Census\n",
        "import pygris\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure OSMnx\n",
        "ox.settings.log_console = False\n",
        "ox.settings.use_cache = True\n",
        "\n",
        "print(\"✓ Libraries imported\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Google Earth Engine\n",
        "# This will open a browser window for authentication\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=GEE_PROJECT_ID)\n",
        "\n",
        "print(\"✓ Google Earth Engine initialized\")"
      ],
      "metadata": {
        "id": "gee_auth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load Your Data"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your O-D pairs\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "\n",
        "print(f\"✓ Loaded {len(df):,} O-D pairs\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "load_data_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify required columns exist\n",
        "required_cols = [COL_ORIGIN_LAT, COL_ORIGIN_LON, COL_DEST_LAT, COL_DEST_LON]\n",
        "missing = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\\n\"\n",
        "                     f\"Available columns: {list(df.columns)}\\n\"\n",
        "                     f\"Please update the column name configuration above.\")\n",
        "else:\n",
        "    print(\"✓ All required columns found\")"
      ],
      "metadata": {
        "id": "verify_cols"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Generate Network Predictors\n",
        "\n",
        "This step calculates:\n",
        "- **Straight_Line_Distance_m** - Haversine distance\n",
        "- **Origin_Road_Length_Density_m_km2** - Road density at origin\n",
        "- **Dest_Intersection_Density_n_km2** - Intersection density at destination\n",
        "\n",
        "⏱️ This may take a while for large datasets due to OSM API queries."
      ],
      "metadata": {
        "id": "network_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def area_km2(geom3857):\n",
        "    \"\"\"Return area of a projected geometry (EPSG:3857) in km²\"\"\"\n",
        "    return geom3857.area / 1_000_000\n",
        "\n",
        "def buffer_from_latlon(lat, lon):\n",
        "    \"\"\"Create a 400m buffer around a point in metric CRS\"\"\"\n",
        "    pt = Point(lon, lat)\n",
        "    return gpd.GeoSeries([pt], crs=4326).to_crs(CRS_METRIC).buffer(BUFFER_M)[0]\n",
        "\n",
        "def calculate_haversine(row):\n",
        "    \"\"\"Calculate haversine distance between origin and destination\"\"\"\n",
        "    origin = (row[COL_ORIGIN_LAT], row[COL_ORIGIN_LON])\n",
        "    destination = (row[COL_DEST_LAT], row[COL_DEST_LON])\n",
        "    return geodesic(origin, destination).meters\n",
        "\n",
        "def net_metrics(buf_metric):\n",
        "    \"\"\"\n",
        "    Compute network predictors inside the buffer (EPSG:3857):\n",
        "      - intersect_density: true ≥3-way intersections per km²\n",
        "      - road_len_density: total walkable road length (m) per km²\n",
        "    \"\"\"\n",
        "    intersect_density = road_len_density = 0.0\n",
        "    area = area_km2(buf_metric)\n",
        "    \n",
        "    # Re-project buffer to WGS84 for OSMnx queries\n",
        "    buf_wgs = gpd.GeoSeries([buf_metric], crs=CRS_METRIC).to_crs(4326).iloc[0]\n",
        "    \n",
        "    try:\n",
        "        G = ox.graph_from_polygon(\n",
        "            buf_wgs,\n",
        "            network_type=\"drive_service\",\n",
        "            simplify=True,\n",
        "            retain_all=False\n",
        "        )\n",
        "        \n",
        "        if len(G.nodes) and len(G.edges):\n",
        "            nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
        "            \n",
        "            # Road length density\n",
        "            edges_m = edges.to_crs(CRS_METRIC)\n",
        "            road_len_m = edges_m.geometry.length.sum()\n",
        "            road_len_density = road_len_m / area\n",
        "            \n",
        "            # Intersection density\n",
        "            nodes_m = nodes.to_crs(CRS_METRIC)\n",
        "            nodes_cl = gpd.clip(nodes_m, buf_metric)\n",
        "            \n",
        "            if \"street_count\" in nodes_cl.columns:\n",
        "                inter_cnt = (nodes_cl[\"street_count\"] >= 3).sum()\n",
        "            else:\n",
        "                deg = Counter([u for u, v, k in G.edges(keys=True)] +\n",
        "                            [v for u, v, k in G.edges(keys=True)])\n",
        "                inter_cnt = sum(1 for n in nodes_cl.index if deg.get(n, 0) >= 3)\n",
        "            \n",
        "            intersect_density = inter_cnt / area\n",
        "            \n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    return intersect_density, road_len_density\n",
        "\n",
        "print(\"✓ Network metric functions defined\")"
      ],
      "metadata": {
        "id": "network_funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate straight-line distance\n",
        "print(\"Calculating straight-line distances...\")\n",
        "tqdm.pandas(desc=\"Computing distances\")\n",
        "df['Straight_Line_Distance_m'] = df.progress_apply(calculate_haversine, axis=1)\n",
        "\n",
        "print(f\"\\n✓ Distance range: [{df['Straight_Line_Distance_m'].min():.1f}, \"\n",
        "      f\"{df['Straight_Line_Distance_m'].max():.1f}] meters\")"
      ],
      "metadata": {
        "id": "calc_distance"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build 400m buffers\n",
        "print(\"Building 400m buffers...\")\n",
        "\n",
        "tqdm.pandas(desc=\"Origin buffers\")\n",
        "df['buf_origin'] = df.progress_apply(\n",
        "    lambda r: buffer_from_latlon(r[COL_ORIGIN_LAT], r[COL_ORIGIN_LON]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "tqdm.pandas(desc=\"Destination buffers\")\n",
        "df['buf_dest'] = df.progress_apply(\n",
        "    lambda r: buffer_from_latlon(r[COL_DEST_LAT], r[COL_DEST_LON]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Buffers created\")"
      ],
      "metadata": {
        "id": "build_buffers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate network metrics\n",
        "print(f\"Processing network metrics for {len(df):,} O-D pairs...\")\n",
        "print(f\"This may take a while. Progress will be shown below.\")\n",
        "\n",
        "# Initialize columns\n",
        "df['Origin_Road_Length_Density_m_km2'] = 0.0\n",
        "df['Origin_Intersection_Density_n_km2'] = 0.0\n",
        "df['Dest_Road_Length_Density_m_km2'] = 0.0\n",
        "df['Dest_Intersection_Density_n_km2'] = 0.0\n",
        "\n",
        "def process_row(i, row):\n",
        "    \"\"\"Wrapper for parallel processing\"\"\"\n",
        "    try:\n",
        "        id_o, rd_o = net_metrics(row.buf_origin)\n",
        "        id_d, rd_d = net_metrics(row.buf_dest)\n",
        "        return i, id_o, rd_o, id_d, rd_d, None\n",
        "    except Exception as e:\n",
        "        return i, 0.0, 0.0, 0.0, 0.0, str(e)\n",
        "\n",
        "# Process in batches with progress bar\n",
        "total = len(df)\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "\n",
        "with tqdm(total=total, desc=\"Processing network metrics\") as pbar:\n",
        "    for batch_start in range(0, total, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total)\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            futures = {\n",
        "                executor.submit(process_row, i, df.iloc[i]): i\n",
        "                for i in range(batch_start, batch_end)\n",
        "            }\n",
        "            \n",
        "            for fut in as_completed(futures):\n",
        "                i, id_o, rd_o, id_d, rd_d, err = fut.result()\n",
        "                if err:\n",
        "                    error_count += 1\n",
        "                else:\n",
        "                    df.iloc[i, df.columns.get_loc('Origin_Intersection_Density_n_km2')] = id_o\n",
        "                    df.iloc[i, df.columns.get_loc('Origin_Road_Length_Density_m_km2')] = rd_o\n",
        "                    df.iloc[i, df.columns.get_loc('Dest_Intersection_Density_n_km2')] = id_d\n",
        "                    df.iloc[i, df.columns.get_loc('Dest_Road_Length_Density_m_km2')] = rd_d\n",
        "                    success_count += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "print(f\"\\n✓ Network metrics complete\")\n",
        "print(f\"  Success: {success_count:,}\")\n",
        "print(f\"  Errors: {error_count}\")"
      ],
      "metadata": {
        "id": "calc_network"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Generate Topographic Predictors\n",
        "\n",
        "This step calculates:\n",
        "- **Elevation_Difference_m** - Elevation change from origin to destination\n",
        "- **Slope_Pct** - Slope as a percentage of horizontal distance"
      ],
      "metadata": {
        "id": "topo_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Earth Engine setup\n",
        "DEM = ee.Image('USGS/SRTMGL1_003')  # 30m SRTM DEM\n",
        "\n",
        "def ee_sample_points(img, points_gdf):\n",
        "    \"\"\"\n",
        "    Sample an Earth Engine image at given point locations.\n",
        "    \"\"\"\n",
        "    fc = ee.FeatureCollection([\n",
        "        ee.Feature(ee.Geometry.Point(pt.x, pt.y))\n",
        "        for pt in points_gdf.geometry\n",
        "    ])\n",
        "    \n",
        "    sampled = img.sampleRegions(\n",
        "        collection=fc,\n",
        "        scale=30,\n",
        "        tileScale=4\n",
        "    )\n",
        "    \n",
        "    band_name = img.bandNames().get(0)\n",
        "    values = sampled.aggregate_array(band_name).getInfo()\n",
        "    return np.array(values, dtype=float)\n",
        "\n",
        "print(\"✓ Topographic functions defined\")"
      ],
      "metadata": {
        "id": "topo_funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process topographic data in batches\n",
        "print(\"Sampling elevation data from SRTM DEM...\")\n",
        "\n",
        "df['Elevation_Origin_m'] = np.nan\n",
        "df['Elevation_Dest_m'] = np.nan\n",
        "df['Elevation_Difference_m'] = np.nan\n",
        "df['Slope_Pct'] = np.nan\n",
        "\n",
        "GEE_BATCH_SIZE = 500  # GEE handles larger batches well\n",
        "total = len(df)\n",
        "\n",
        "with tqdm(total=total, desc=\"Processing elevation data\") as pbar:\n",
        "    for batch_start in range(0, total, GEE_BATCH_SIZE):\n",
        "        batch_end = min(batch_start + GEE_BATCH_SIZE, total)\n",
        "        batch_idx = slice(batch_start, batch_end)\n",
        "        \n",
        "        try:\n",
        "            # Build point geometries\n",
        "            pts_origin = gpd.GeoSeries([\n",
        "                Point(lon, lat) for lon, lat in zip(\n",
        "                    df.loc[batch_idx, COL_ORIGIN_LON],\n",
        "                    df.loc[batch_idx, COL_ORIGIN_LAT]\n",
        "                )\n",
        "            ], crs=4326)\n",
        "            \n",
        "            pts_dest = gpd.GeoSeries([\n",
        "                Point(lon, lat) for lon, lat in zip(\n",
        "                    df.loc[batch_idx, COL_DEST_LON],\n",
        "                    df.loc[batch_idx, COL_DEST_LAT]\n",
        "                )\n",
        "            ], crs=4326)\n",
        "            \n",
        "            # Sample elevation\n",
        "            elev_origin = ee_sample_points(DEM, gpd.GeoDataFrame(geometry=pts_origin))\n",
        "            elev_dest = ee_sample_points(DEM, gpd.GeoDataFrame(geometry=pts_dest))\n",
        "            \n",
        "            # Calculate derived metrics\n",
        "            delta_elev = elev_dest - elev_origin\n",
        "            euclidean_dist = df.loc[batch_idx, 'Straight_Line_Distance_m'].values\n",
        "            euclidean_dist = np.where(euclidean_dist == 0, np.nan, euclidean_dist)\n",
        "            slope_pct = (delta_elev / euclidean_dist) * 100\n",
        "            \n",
        "            # Assign results\n",
        "            df.loc[batch_idx, 'Elevation_Origin_m'] = elev_origin\n",
        "            df.loc[batch_idx, 'Elevation_Dest_m'] = elev_dest\n",
        "            df.loc[batch_idx, 'Elevation_Difference_m'] = delta_elev\n",
        "            df.loc[batch_idx, 'Slope_Pct'] = slope_pct\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nWarning: Error in batch {batch_start}-{batch_end}: {e}\")\n",
        "        \n",
        "        pbar.update(batch_end - batch_start)\n",
        "\n",
        "print(\"\\n✓ Topographic data complete\")"
      ],
      "metadata": {
        "id": "calc_topo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Generate Demographic Predictors\n",
        "\n",
        "This step calculates:\n",
        "- **Population** - Census block group population at origin\n",
        "\n",
        "⚠️ This uses US Census data. If your study area is outside the US, you'll need to substitute an appropriate population data source."
      ],
      "metadata": {
        "id": "demo_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Census block group boundaries\n",
        "print(f\"Downloading Census block groups for State {STATE_FIPS}, County {COUNTY_FIPS}...\")\n",
        "\n",
        "try:\n",
        "    block_groups = pygris.block_groups(\n",
        "        state=STATE_FIPS,\n",
        "        county=COUNTY_FIPS,\n",
        "        year=CENSUS_YEAR,\n",
        "        cache=True\n",
        "    )\n",
        "    block_groups = block_groups.to_crs(CRS_METRIC)\n",
        "    print(f\"✓ Downloaded {len(block_groups)} block groups\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading block groups: {e}\")\n",
        "    print(\"Check your FIPS codes and internet connection.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "download_census"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch population data from Census API\n",
        "print(\"Fetching population data from Census API...\")\n",
        "\n",
        "try:\n",
        "    c = Census(CENSUS_API_KEY)\n",
        "    \n",
        "    pop_data = c.pl.get(\n",
        "        ('P1_001N',),  # Total population\n",
        "        geo={\n",
        "            'for': 'block group:*',\n",
        "            'in': f'state:{STATE_FIPS} county:{COUNTY_FIPS}'\n",
        "        },\n",
        "        year=CENSUS_YEAR\n",
        "    )\n",
        "    \n",
        "    pop_df = pd.DataFrame(pop_data)\n",
        "    pop_df = pop_df.rename(columns={'P1_001N': 'Population'})\n",
        "    pop_df['GEOID'] = (\n",
        "        pop_df['state'] +\n",
        "        pop_df['county'] +\n",
        "        pop_df['tract'] +\n",
        "        pop_df['block group']\n",
        "    )\n",
        "    pop_df = pop_df[['GEOID', 'Population']].astype({'Population': float})\n",
        "    \n",
        "    print(f\"✓ Population range: [{pop_df['Population'].min():.0f}, {pop_df['Population'].max():.0f}]\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error fetching Census data: {e}\")\n",
        "    print(\"Check your Census API key.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "fetch_pop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spatial join to assign population to each origin\n",
        "print(\"Performing spatial join...\")\n",
        "\n",
        "# Merge population with block group geometries\n",
        "block_groups = block_groups.merge(pop_df, on='GEOID', how='left')\n",
        "block_groups['Population'] = block_groups['Population'].fillna(0)\n",
        "\n",
        "# Create GeoDataFrame of origin points\n",
        "geometry = [\n",
        "    Point(lon, lat)\n",
        "    for lon, lat in zip(df[COL_ORIGIN_LON], df[COL_ORIGIN_LAT])\n",
        "]\n",
        "gdf_origins = gpd.GeoDataFrame(df, geometry=geometry, crs=4326)\n",
        "gdf_origins = gdf_origins.to_crs(CRS_METRIC)\n",
        "\n",
        "# Spatial join\n",
        "gdf_joined = gdf_origins.sjoin(\n",
        "    block_groups[['GEOID', 'Population', 'geometry']],\n",
        "    how='left',\n",
        "    predicate='within'\n",
        ")\n",
        "\n",
        "df['Population'] = gdf_joined['Population'].values\n",
        "df['Population'] = df['Population'].fillna(0)\n",
        "\n",
        "n_matched = (df['Population'] > 0).sum()\n",
        "print(f\"✓ Matched {n_matched:,} / {len(df):,} origins to block groups\")"
      ],
      "metadata": {
        "id": "spatial_join"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Save Results"
      ],
      "metadata": {
        "id": "save_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the 6 required predictor columns plus original data\n",
        "predictor_cols = [\n",
        "    'Straight_Line_Distance_m',\n",
        "    'Origin_Road_Length_Density_m_km2',\n",
        "    'Dest_Intersection_Density_n_km2',\n",
        "    'Slope_Pct',\n",
        "    'Elevation_Difference_m',\n",
        "    'Population'\n",
        "]\n",
        "\n",
        "# Drop temporary columns\n",
        "cols_to_drop = ['buf_origin', 'buf_dest', 'Elevation_Origin_m', 'Elevation_Dest_m',\n",
        "                'Origin_Intersection_Density_n_km2', 'Dest_Road_Length_Density_m_km2']\n",
        "df_final = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "# Save to CSV\n",
        "df_final.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTOR GENERATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nSaved to: {OUTPUT_CSV}\")\n",
        "print(f\"Records: {len(df_final):,}\")\n",
        "print(f\"\\nPredictor Summary:\")\n",
        "print(\"-\"*60)\n",
        "for col in predictor_cols:\n",
        "    if col in df_final.columns:\n",
        "        print(f\"{col:40} Mean: {df_final[col].mean():10.2f}\")\n",
        "print(\"-\"*60)\n",
        "print(\"\\n✓ Ready for model prediction!\")"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file (Colab)\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_CSV)"
      ],
      "metadata": {
        "id": "download_file"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
